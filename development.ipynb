{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b8285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional, Dict, List, Union, Any\n",
    "\n",
    "import math \n",
    "import random\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import JAX\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.nn.utils.rnn import pack_sequence, pad_sequence, pack_padded_sequence, pad_packed_sequence \n",
    "import torch.nn.functional as F \n",
    "\n",
    "from einops import rearrange \n",
    "\n",
    "from flash_attn.flash_attention import FlashMHA, FlashAttention \n",
    "from flash_attn.bert_padding import unpad_input, pad_input \n",
    "from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    '''\n",
    "    Positional encoding for transformer models.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1) -> None: \n",
    "        '''\n",
    "        Initialize the positional encoding layer. \n",
    "\n",
    "        Args:\n",
    "            d_model (int): The dimension of the model embeddings\n",
    "            max_len (int): Maximum sequence length for positional encoding\n",
    "            dropout (float): Dropout probability for the positional encoding\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(\n",
    "            max_len, \n",
    "            d_model\n",
    "        )\n",
    "        position = torch.arange(\n",
    "            start=0,\n",
    "            end=max_len,\n",
    "            dtype=torch.float\n",
    "        ).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * \n",
    "            (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: \n",
    "        '''\n",
    "        Apply positional encoding to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (seq_len, batch_size, d_model)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor with the positional encoding added\n",
    "        '''\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f85a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFlashAttention(nn.Module):\n",
    "    '''\n",
    "    Custom Flash Attention optimized to work with Tesla GPUs.\n",
    "    '''\n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.1) -> None:\n",
    "        '''\n",
    "        Initialize the custom flash attention module.\n",
    "\n",
    "        Args:\n",
    "            embed_dim (int): The embedding dimension \n",
    "            num_heads (int): Number of attention heads \n",
    "            dropout (float): Dropout probability for attention weights\n",
    "\n",
    "        Returns: \n",
    "            None\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads \n",
    "        self.dropout_p = dropout\n",
    "        self.softmax_scale = embed_dim ** -0.5\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        q: torch.Tensor, \n",
    "        k: torch.Tensor, \n",
    "        v: torch.Tensor,\n",
    "        key_padding_mask: Optional[torch.Tensor],\n",
    "        casual: bool = False,\n",
    "        cu_seqlens: Optional[torch.Tensor] = None,\n",
    "        max_s: Optional[int] = None,\n",
    "        need_weights: bool = False\n",
    "        ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        '''\n",
    "        Implements the multihead softmax attention with separate query, key and value tensors.\n",
    "\n",
    "        Args:\n",
    "            q (torch.Tensor): Query tensor of shape (B, S_q, H, D)\n",
    "            k (torch.Tensor): Key tensor of shape (B, S_k, H, D)\n",
    "            v (torch.Tensor): Value tensor of shape (B, S_v, H, D)\n",
    "            key_padding_mask (Optional[torch.Tensor]): Boolean mask for padding, shape (B, S_k)\n",
    "            casual (bool): Whether to use causal attention\n",
    "            cu_seqlens (Optional[torch.Tensor]): Cumulative sequence lengths for packed sequences\n",
    "            max_s (Optional[int]): Maximum sequence length\n",
    "            need_weights (bool): Whether to return attention weights\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, Optional[torch.Tensor]]: Output tensor and optional attention weights\n",
    "        '''\n",
    "        assert q.dtype in [torch.float16, torch.bfloat16]\n",
    "        assert q.dtype == k.dtype == v.dtype \n",
    "        assert q.is_cuda and k.is_cuda and v.is_cuda \n",
    "\n",
    "        batch_size = q.shape[0]\n",
    "        seqlen_q = q.shape[1]\n",
    "        seqlen_k = k.shape[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
